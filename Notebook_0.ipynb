{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Pytorch Fundamentals\n",
    "\n",
    "Resource notebook: https://www.learnpytorch.io/00-pytorch-fundamentals/\n",
    "\n",
    "If you have a question: https://github.com/mrdbourke/pytorch-deep-learning/discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 26 03:14:07 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.70                 Driver Version: 560.70         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   69C    P0             24W /   80W |    1824MiB /   6144MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2684    C+G   ...0.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A      4408    C+G   ...ata\\Local\\Box\\Box Edit\\Box Edit.exe      N/A      |\n",
      "|    0   N/A  N/A      5296    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      7960    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A      9144    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     10292    C+G   C:\\Windows\\System32\\NahimicSvc64.exe        N/A      |\n",
      "|    0   N/A  N/A     12912    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A     13036    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     13388    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     13660    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     14356    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     15560    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     15628    C+G   ...mpt_builder\\LogiAiPromptBuilder.exe      N/A      |\n",
      "|    0   N/A  N/A     16276    C+G   ...aam7r\\AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A     17232    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     18152    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     19784    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19836    C+G   ...werToys\\PowerToys.PowerLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     20060    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     20220    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     20280    C+G   ...ys\\WinUI3Apps\\PowerToys.Peek.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     22280    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22544    C+G   ...pIntegrations\\Grammarly.Desktop.exe      N/A      |\n",
      "|    0   N/A  N/A     23188    C+G   ...BridgeWPF\\SamsungNotesBridgeWPF.exe      N/A      |\n",
      "|    0   N/A  N/A     23460    C+G   ...e Stream\\94.0.1.0\\GoogleDriveFS.exe      N/A      |\n",
      "|    0   N/A  N/A     26120    C+G   ...1.0_x64__w2gh52qy24etm\\Nahimic3.exe      N/A      |\n",
      "|    0   N/A  N/A     28884    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A     29292    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     29636    C+G   ...ekyb3d8bbwe\\WsaClient\\WsaClient.exe      N/A      |\n",
      "|    0   N/A  N/A     30976    C+G   ....5536.0_x64__8j3eq9eme6ctt\\IGCC.exe      N/A      |\n",
      "|    0   N/A  N/A     31912    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tensors\n",
    "\n",
    "### Creating Tensors\n",
    "\n",
    "PyTorch tensors are created using `torch.Tensor()` = https://pytorch.org/docs/stable/tensors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim  # Here the dimension is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tensor back as Python Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MATRIX\n",
    "MATRIX = torch.tensor([[1,2],\n",
    "                       [3,4]])\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TENSOR\n",
    "TENSOR = torch.tensor([[[1,2,3],\n",
    "                        [4,5,6],\n",
    "                        [7,8,9]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tensors\n",
    "\n",
    "\n",
    "Why random tensors?\n",
    "Random tensors are important because the way many neural networks learn is that they start with tensors full of random numbers and then adjust those random to better represent the data.\n",
    "\n",
    "`Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers -> ....`\n",
    "\n",
    "\n",
    "Torch random tensors â€” https://pytorch.org/docs/stable/generated/torch.rand.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7321, 0.2951, 0.1870, 0.2228],\n",
       "        [0.6234, 0.1095, 0.8023, 0.3097],\n",
       "        [0.4184, 0.6193, 0.4543, 0.2422]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random Tensor of size (3,4)\n",
    "random_tensor = torch.rand(3,4)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.3396e-01, 7.9041e-01, 2.1471e-01, 8.5110e-01, 5.1757e-01,\n",
       "          7.0966e-01, 8.8520e-01, 9.1851e-03, 9.1606e-01, 2.7355e-01],\n",
       "         [9.2171e-01, 8.9553e-01, 3.7225e-01, 3.1633e-01, 8.6061e-01,\n",
       "          5.8490e-01, 7.0734e-01, 6.2798e-01, 3.5427e-02, 4.8310e-01],\n",
       "         [3.1074e-01, 5.3273e-01, 8.0227e-02, 2.3559e-01, 7.7160e-01,\n",
       "          2.0782e-01, 2.0592e-01, 4.8041e-01, 3.2458e-01, 7.2269e-01],\n",
       "         [9.0525e-01, 3.6679e-01, 5.1672e-01, 7.2491e-04, 7.2613e-01,\n",
       "          7.5330e-01, 5.8603e-01, 3.1638e-01, 3.6637e-01, 4.8880e-01],\n",
       "         [8.4685e-01, 2.9329e-01, 8.5564e-01, 4.6222e-01, 9.9438e-01,\n",
       "          5.8080e-01, 9.9739e-01, 7.1093e-02, 9.3276e-01, 2.4454e-02],\n",
       "         [6.8351e-01, 1.0705e-01, 4.7978e-01, 2.6708e-01, 2.1989e-01,\n",
       "          3.8960e-01, 3.3552e-02, 2.6794e-02, 5.4780e-01, 6.2815e-01],\n",
       "         [8.9816e-01, 8.0556e-01, 7.9739e-01, 8.1022e-02, 1.6180e-01,\n",
       "          4.7095e-01, 2.2615e-01, 8.4481e-01, 5.3238e-01, 1.4817e-01],\n",
       "         [3.3514e-01, 5.7417e-01, 2.0595e-01, 3.3877e-01, 3.0053e-01,\n",
       "          8.6967e-01, 2.8592e-01, 2.7675e-01, 9.9546e-01, 4.0726e-01],\n",
       "         [8.1552e-01, 4.7446e-01, 3.1146e-01, 4.4861e-01, 9.3500e-01,\n",
       "          3.9320e-01, 9.9989e-01, 2.0679e-01, 3.0103e-01, 2.1659e-01],\n",
       "         [1.0404e-02, 1.9008e-01, 3.3513e-01, 5.1067e-01, 3.0434e-01,\n",
       "          2.6984e-01, 8.8517e-01, 9.2849e-01, 1.4392e-01, 2.6606e-01]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor1 = torch.rand(1,10,10)\n",
    "random_tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor1.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6211, 0.3794, 0.9431],\n",
       "         [0.8770, 0.5937, 0.2930],\n",
       "         [0.2141, 0.2615, 0.2168],\n",
       "         ...,\n",
       "         [0.0974, 0.7568, 0.7583],\n",
       "         [0.7039, 0.3746, 0.4803],\n",
       "         [0.9881, 0.7511, 0.4480]],\n",
       "\n",
       "        [[0.4965, 0.6123, 0.3493],\n",
       "         [0.8497, 0.4556, 0.5724],\n",
       "         [0.8613, 0.4687, 0.6528],\n",
       "         ...,\n",
       "         [0.3083, 0.4904, 0.7180],\n",
       "         [0.7699, 0.8317, 0.8199],\n",
       "         [0.4525, 0.8840, 0.9061]],\n",
       "\n",
       "        [[0.1574, 0.9924, 0.8202],\n",
       "         [0.7456, 0.7644, 0.2544],\n",
       "         [0.4838, 0.3403, 0.3302],\n",
       "         ...,\n",
       "         [0.7689, 0.4766, 0.8564],\n",
       "         [0.5958, 0.9789, 0.7124],\n",
       "         [0.0962, 0.6236, 0.1098]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.4803, 0.3042, 0.0118],\n",
       "         [0.2624, 0.1307, 0.9448],\n",
       "         [0.7208, 0.4117, 0.9322],\n",
       "         ...,\n",
       "         [0.0844, 0.6836, 0.2838],\n",
       "         [0.4491, 0.9760, 0.8202],\n",
       "         [0.2131, 0.4953, 0.6704]],\n",
       "\n",
       "        [[0.8541, 0.6910, 0.3836],\n",
       "         [0.4714, 0.9752, 0.8164],\n",
       "         [0.6028, 0.2542, 0.8986],\n",
       "         ...,\n",
       "         [0.8577, 0.7009, 0.9793],\n",
       "         [0.1492, 0.0636, 0.5189],\n",
       "         [0.7461, 0.7049, 0.6280]],\n",
       "\n",
       "        [[0.8443, 0.1168, 0.2236],\n",
       "         [0.0566, 0.0535, 0.6511],\n",
       "         [0.0652, 0.0472, 0.0298],\n",
       "         ...,\n",
       "         [0.4803, 0.8154, 0.0150],\n",
       "         [0.2515, 0.3085, 0.5961],\n",
       "         [0.2309, 0.4200, 0.1505]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a random image tensor\n",
    "random_image = torch.rand(size = (224,224,3)) # 224*224 is the height and width of the image and 3 is number of colour channels.\n",
    "random_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 224, 3]), 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_image.shape, random_image.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeros and Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(size=(3,4))\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(size=(3,4))\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones.dtype,zeros.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a range of tensors and tensors-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.arange()\n",
    "one_to_ten = torch.arange(1,11)\n",
    "one_to_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,  101,  201,  301,  401,  501,  601,  701,  801,  901, 1001])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_tensor  = torch.arange(start=1,end=1002,step=100)\n",
    "step_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Tensors Like\n",
    "ten_zeros = torch.zeros_like(input = one_to_ten)\n",
    "ten_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":## Tensor Datatypes\n",
    "\n",
    "**Note:** Tensor datatype is the 3 big errors you will run into with pytorch and deep learning:\n",
    "\n",
    "1. Tensors not the right datatype\n",
    "2. Tensors not the right shape\n",
    "3. Tensors not on the right device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor = torch.tensor([3.0,6.0,9.0],\n",
    "                               dtype = torch.float32, # What is the datatype of the tensor? (eg. float32, float16 or float64, etc) Default is float32\n",
    "                               device = \"cuda\",  # What device is your tensor on?\n",
    "                               requires_grad = True) # Whether or not to track gradients with this tensor's operations\n",
    "\n",
    "float_32_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor = float_32_tensor.type(torch.float16)\n",
    "float_16_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 36., 81.], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor * float_16_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_32_tensor = torch.tensor([1,2,3],\n",
    "                             dtype = torch.int32,\n",
    "                             device = \"cuda\",\n",
    "                             requires_grad = False)\n",
    "\n",
    "int_32_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3., 12., 27.], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor * int_32_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting information from tensors (Tensor Attributes)\n",
    "\n",
    "1. Tensors not the right datatype - To get the datatype of the tensor you can use `tensor.dtype`\n",
    "2. Tensors not the right shape - to get the shape of the tensor you can use `tensor.shape`\n",
    "3. Tensors not on the right device - to get the device of the tensor you can use `tensor.device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int32\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(float_32_tensor.dtype)\n",
    "\n",
    "print(int_32_tensor.dtype)\n",
    "\n",
    "print(float_32_tensor.shape)\n",
    "\n",
    "print(int_32_tensor.shape)\n",
    "\n",
    "print(float_32_tensor.device)\n",
    "\n",
    "print(int_32_tensor.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4751, 0.4115, 0.1705, 0.0481, 0.3471],\n",
       "        [0.4849, 0.8767, 0.8949, 0.4659, 0.1045],\n",
       "        [0.9348, 0.3080, 0.5424, 0.9775, 0.5438],\n",
       "        [0.6038, 0.8251, 0.8767, 0.5685, 0.9427]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor = torch.rand(4,5)\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of the tensor: torch.float32\n",
      "Shape of the tensor : torch.Size([4, 5])\n",
      "Shape of the tensor : torch.Size([4, 5])\n",
      "Device tensor is stored on : cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Datatype of the tensor: {some_tensor.dtype}\")\n",
    "print(f\"Shape of the tensor : {some_tensor.shape}\")\n",
    "print(f\"Shape of the tensor : {some_tensor.size()}\")\n",
    "print(f\"Device tensor is stored on : {some_tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Tensors (Tensor operations)\n",
    "\n",
    "Tensor Operations include:\n",
    "1. Addition\n",
    "2. Subtraction\n",
    "3. Multiplication (Element-wise)\n",
    "4. Division\n",
    "5. Matrix Multiplication (Dot Product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor and add 10 to it\n",
    "\n",
    "tensor = torch.tensor([1,2,3])\n",
    "tensor = tensor + 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other way\n",
    "\n",
    "tensor = torch.tensor([1,2,3])\n",
    "tensor = torch.add(tensor,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 15, 25])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor and subtract 10 from it\n",
    "tensor = torch.tensor([10,20,30])\n",
    "tensor = tensor - 5\n",
    "tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 10, 15])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplying a tensor\n",
    "\n",
    "tensor = torch.tensor([1,2,3])\n",
    "tensor = tensor * 5\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other way\n",
    "\n",
    "tensor = torch.tensor([1,2,3])\n",
    "tensor = torch.mul(tensor,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "There are 2 main ways of performing multiplication in neural networks and deep learning.\n",
    "\n",
    "1. Element wise multiplication\n",
    "2. Matrix multiplication (dot product)\n",
    "\n",
    "More information on multiplying matrices - https://www.mathsisfun.com/algebra/matrix-multiplying.html\n",
    "\n",
    "There are two main rules that Matrix mutliplication needs to satisfy:\n",
    "1. The inner dimensions must match:\n",
    "* `(3,2) @ (2,3)` will work\n",
    "* `(3,2) @ (3,2)` will not work\n",
    "* `(2,3) @ (3,2)` will work\n",
    "* `(2,3) @ (2,3)` will not work\n",
    "* The numbers above are dimensions of the matrix\n",
    "\n",
    "2. The reulting matrix has a shape of outer dimensions\n",
    "* `(3,2) @ (2,3)` will result in a matrix of dimension `(3,3)`\n",
    "* `(2,3) @ (3,2)` will result in a matrix of dimension `(2,2)`\n",
    "\n",
    "Website showing how matrix multiplication is performed - http://matrixmultiplication.xyz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])  *  tensor([2, 3, 4])\n",
      "Equals :  tensor([ 4,  9, 16])\n"
     ]
    }
   ],
   "source": [
    "# element wise multiplication\n",
    "\n",
    "tensor = torch.tensor([2,3,4])\n",
    "\n",
    "print(tensor, \" * \", tensor)\n",
    "print(\"Equals : \", tensor * tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "\n",
    "matrix_mul = torch.matmul(tensor, tensor)\n",
    "matrix_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One of the most common errors in deep learning: shape errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapes for matrix multiplication\n",
    "\n",
    "tensor_A = torch.tensor([[1,2],\n",
    "                         [3,4],\n",
    "                         [5,6]])\n",
    "\n",
    "tensor_B = torch.tensor([[7,10],\n",
    "                         [8,11],\n",
    "                         [9,12]])\n",
    "\n",
    "#torch.matmul(tensor_A, tensor_B)  ## This will throw an error because both the tensors don't have the same inner dimension.\n",
    "\n",
    "#torch.mm(tensor_A, tensor_B) ## torch.mm is an alias for torch.matmul. This will also throw an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix our tensor shape issues, we can manipulate the shape of one of our tensors using a **transpose**.\n",
    "\n",
    "\n",
    "A **transpose** switches the axes or dimensions of a given tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7, 10],\n",
       "         [ 8, 11],\n",
       "         [ 9, 12]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_B, tensor_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7,  8,  9],\n",
       "         [10, 11, 12]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_B.T, tensor_B.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes --> tensor_A = torch.Size([3, 2]) , tensor_B = torch.Size([3, 2])\n",
      "New shapes --> tensor_A = torch.Size([3, 2]) , tensor_B = torch.Size([2, 3])\n",
      "Multiplying torch.Size([3, 2]) @ torch.Size([3, 2]) (Same shape as above)  <-- Inner dimmensions must match here.\n",
      "Output : \n",
      "\n",
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n",
      "\n",
      "Output shape is : torch.Size([3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 27,  30,  33],\n",
       "        [ 61,  68,  75],\n",
       "        [ 95, 106, 117]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The matrix multiplication operation of tensor_B works when we take a transpose of tensor_B\n",
    "\n",
    "print(f\"Original shapes --> tensor_A = {tensor_A.shape} , tensor_B = {tensor_B.shape}\")\n",
    "print(f\"New shapes --> tensor_A = {tensor_A.shape} , tensor_B = {tensor_B.T.shape}\")\n",
    "print(f\"Multiplying {tensor_A.shape} @ {tensor_B.shape} (Same shape as above)  <-- Inner dimmensions must match here.\")\n",
    "print(f\"Output : \\n\")\n",
    "output = torch.matmul(tensor_A,tensor_B.T)\n",
    "print(output)\n",
    "print(f\"\\nOutput shape is : {output.shape}\")\n",
    "\n",
    "torch.mm(tensor_A,tensor_B.T)  ## This is another way to perform the same operation using the torch.nn alias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Aggregation\n",
    "\n",
    "Finding the min, max, mean, sum, etc of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0,100,10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(0))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(x), x.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(90), tensor(90))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x), x.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Average/Mean\n",
    "\n",
    "`Note`: The torch.mean()/.mean() function requires a dataype of float32. Or else it throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(45.), tensor(45.))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(x.type(torch.float32)), x.type(torch.float32).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(450), tensor(450))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x), x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional min and max\n",
    "\n",
    "`torch.argmin()/.argmin()` and `torch.argmax()/.argmax()` function return the position/index of the min/max of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(0))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(x), x.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9), tensor(9))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(x),x.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping, stacking, squeezing and unsqueezing tensors\n",
    "\n",
    "* Reshaping - reshapes an input tensor to a defined shape\n",
    "* View - Return a view of an input tensor of certain shape but keep the same memory as the original tensor\n",
    "* Stacking - combine multiple tensors on top of each other (vstack) or side by side (hstack)\n",
    "* Squeeze - removes all `1` dimensions from a tensor\n",
    "* Unsqueeze - add a `1` dimension to a target tensor\n",
    "* Permute - Return a view of the input with dimensions permuted (swapped) in a certain way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]), torch.Size([10]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(1.,11.,1)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]),\n",
       " torch.Size([1, 10]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped = torch.reshape(x,(1,10))\n",
    "reshaped, reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.],\n",
       "         [ 2.],\n",
       "         [ 3.],\n",
       "         [ 4.],\n",
       "         [ 5.],\n",
       "         [ 6.],\n",
       "         [ 7.],\n",
       "         [ 8.],\n",
       "         [ 9.],\n",
       "         [10.]]),\n",
       " torch.Size([10, 1]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped = x.reshape(10,1)  # this is the same function as above just a different way of calling the method.\n",
    "reshaped, reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.],\n",
       "         [ 3.,  4.],\n",
       "         [ 5.,  6.],\n",
       "         [ 7.,  8.],\n",
       "         [ 9., 10.]]),\n",
       " torch.Size([5, 2]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped = x.reshape(5,2)\n",
    "reshaped, reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9., 10.]]),\n",
       " torch.Size([2, 5]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped = x.reshape(2,5)\n",
    "reshaped, reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]),\n",
       " torch.Size([1, 10]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the view\n",
    "\n",
    "z= x.view(1,10)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing `z` changes `x` because the view of a tensor occupies the same memory as the original tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]),\n",
       " tensor([ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:,0] = 5\n",
    "z , x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "        [ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "        [ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "        [ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "        [ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stacking the tensors on top of each other\n",
    "\n",
    "x_stacked = torch.stack([x,x,x,x,x],dim = 0)\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  5.,  5.,  5.,  5.],\n",
       "        [ 2.,  2.,  2.,  2.,  2.],\n",
       "        [ 3.,  3.,  3.,  3.,  3.],\n",
       "        [ 4.,  4.,  4.,  4.,  4.],\n",
       "        [ 5.,  5.,  5.,  5.,  5.],\n",
       "        [ 6.,  6.,  6.,  6.,  6.],\n",
       "        [ 7.,  7.,  7.,  7.,  7.],\n",
       "        [ 8.,  8.,  8.,  8.,  8.],\n",
       "        [ 9.,  9.,  9.,  9.,  9.],\n",
       "        [10., 10., 10., 10., 10.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stacking the tensors on top of each other on the other dimension\n",
    "\n",
    "x_stacked = torch.stack([x,x,x,x,x],dim = 1)\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]]),\n",
       " torch.Size([1, 1, 10]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x, x.shape)\n",
    "x = x.reshape(1,1,10)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "squeezed_x = torch.squeeze(x)\n",
    "\n",
    "print(squeezed_x, squeezed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous tensor : tensor([[[ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "shape of previous tensor : torch.Size([1, 1, 10])\n",
      "\n",
      "squeezed tensor : tensor([ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "shape of squeezed tensor : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# torch.squeeze to remove all the single dimensions from a target tensor\n",
    "\n",
    "x = x.reshape(1,1,10)\n",
    "\n",
    "print(f\"previous tensor : {x}\")\n",
    "print(f\"shape of previous tensor : {x.shape}\\n\")\n",
    "\n",
    "squeezed_x = torch.squeeze(x)\n",
    "print(f\"squeezed tensor : {squeezed_x}\")\n",
    "print(f\"shape of squeezed tensor : {squeezed_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous tensor : tensor([[[ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "shape of previous tensor : torch.Size([1, 1, 10])\n",
      "\n",
      "unsqueezed tensor : tensor([[[[ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]]])\n",
      "shape of unsqueezed tensor : torch.Size([1, 1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze adds a single dimension to the target tensor on the specified dim (dimension)\n",
    "\n",
    "print(f\"previous tensor : {x}\")\n",
    "print(f\"shape of previous tensor : {x.shape}\\n\")\n",
    "\n",
    "unsqueezed_x = torch.unsqueeze(x,dim=0) # adding a single dimension at zeroeth dimension\n",
    "print(f\"unsqueezed tensor : {unsqueezed_x}\")\n",
    "print(f\"shape of unsqueezed tensor : {unsqueezed_x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous tensor : tensor([[[ 5.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])\n",
      "shape of previous tensor : torch.Size([1, 1, 10])\n",
      "\n",
      "unsqueezed tensor : tensor([[[[ 5.],\n",
      "          [ 2.],\n",
      "          [ 3.],\n",
      "          [ 4.],\n",
      "          [ 5.],\n",
      "          [ 6.],\n",
      "          [ 7.],\n",
      "          [ 8.],\n",
      "          [ 9.],\n",
      "          [10.]]]])\n",
      "shape of unsqueezed tensor : torch.Size([1, 1, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze adds a single dimension to the target tensor on the specified dim (dimension)\n",
    "\n",
    "print(f\"previous tensor : {x}\")\n",
    "print(f\"shape of previous tensor : {x.shape}\\n\")\n",
    "\n",
    "unsqueezed_x = torch.unsqueeze(x,dim=3) # adding a single dimension at third dimension\n",
    "print(f\"unsqueezed tensor : {unsqueezed_x}\")\n",
    "print(f\"shape of unsqueezed tensor : {unsqueezed_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of previous tensor : torch.Size([224, 224, 3])\n",
      "shape of permuted tensor : torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# torch.permute - rearranges the dimensions of the target tensor to the dimensions specified in specific order\n",
    "# this creates a view of the original tensor. so it uses the same memory.\n",
    "\n",
    "x = torch.rand(224,224,3) # height , width , colour_channels\n",
    "\n",
    "# permute the original tensor to rearrange the axis (or dim) order\n",
    "permuted_x = x.permute(2,0,1) # colour_channels, height, width\n",
    "# Shifts the axis from 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"shape of previous tensor : {x.shape}\")\n",
    "\n",
    "print(f\"shape of permuted tensor : {permuted_x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3182)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3182)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permuted_x[1,0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing - selecting data from tensors\n",
    "\n",
    "Indexing data in pytorch is the same as indexing data with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12],\n",
       "         [13, 14, 15, 16]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "\n",
    "import torch\n",
    "x = torch.arange(1,17).reshape(1,4,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12],\n",
       "        [13, 14, 15, 16]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing on zeroth dimension\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing on first dimension\n",
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing on the second dimension\n",
    "x[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to get the number 9?\n",
    "x[0][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  7, 11, 15]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch tensors and NumPy\n",
    "\n",
    "NumPy is a scientific Python numericalcomputing library\n",
    "\n",
    "And because of this PyTorch has the ability to interact with NumPy.\n",
    "\n",
    "* Data is in NumPy format -> want to convert it to PyTorch Tensors : `torch.from_numpy(ndarray)`\n",
    "* Data is in PyTorch Tensor format -> want to convert it to NumPy : `torch.Tensor.numpy()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  5  9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93\n",
      " 97] <class 'numpy.ndarray'> int16\n",
      "tensor([ 1.,  5.,  9., 13., 17., 21., 25., 29., 33., 37., 41., 45., 49., 53.,\n",
      "        57., 61., 65., 69., 73., 77., 81., 85., 89., 93., 97.]) <class 'torch.Tensor'> torch.float32\n"
     ]
    }
   ],
   "source": [
    "# NumPy array to PyTorch tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "numpy_arr = np.arange(1,101,4,dtype=np.int16)\n",
    "print(numpy_arr, type(numpy_arr), numpy_arr.dtype)\n",
    "\n",
    "# note: while converting pytorch retains the original dtype of the numpy array uneless specified otherwise\n",
    "torch_tensor = torch.from_numpy(numpy_arr).type(torch.float32)\n",
    "print(torch_tensor, type(torch_tensor),torch_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  6 10 14 18 22 26 30 34 38 42 46 50 54 58 62 66 70 74 78 82 86 90 94\n",
      " 98]\n",
      "tensor([ 1.,  5.,  9., 13., 17., 21., 25., 29., 33., 37., 41., 45., 49., 53.,\n",
      "        57., 61., 65., 69., 73., 77., 81., 85., 89., 93., 97.])\n"
     ]
    }
   ],
   "source": [
    "# What if we modify the contents of the numpy array. What will happen to the tensor?\n",
    "\n",
    "numpy_arr = numpy_arr + 1\n",
    "print(numpy_arr)\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  5.  9. 13. 17. 21. 25. 29. 33. 37. 41. 45. 49. 53. 57. 61. 65. 69.\n",
      " 73. 77. 81. 85. 89. 93. 97.] <class 'numpy.ndarray'> float32\n"
     ]
    }
   ],
   "source": [
    "# pytorch tensor to numpy array\n",
    "\n",
    "new_numpy_arr = torch.Tensor.numpy(torch_tensor)\n",
    "print(new_numpy_arr, type(new_numpy_arr), new_numpy_arr.dtype)\n",
    "# pytorch also retains the dtype of the original tensor while creating a new numpy array unless specified otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.,  7., 11., 15., 19., 23., 27., 31., 35., 39., 43., 47., 51., 55.,\n",
      "        59., 63., 67., 71., 75., 79., 83., 87., 91., 95., 99.])\n",
      "[ 1.  5.  9. 13. 17. 21. 25. 29. 33. 37. 41. 45. 49. 53. 57. 61. 65. 69.\n",
      " 73. 77. 81. 85. 89. 93. 97.]\n"
     ]
    }
   ],
   "source": [
    "# what happens to the numpy array if we change the original tensor?\n",
    "\n",
    "torch_tensor = torch_tensor + 2\n",
    "print(torch_tensor)\n",
    "print(new_numpy_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducbility (trying to take random out of the random)\n",
    "\n",
    "In short how a neural network learns:\n",
    "\n",
    "`start with random numbers â€”> tensor operations â€”> update random numbers to try and make them better representations of the data â€”> again â€”> again â€”> again..`\n",
    "\n",
    "To reduce the randomness in neural networks and PyTorch comes the concept of a **random seed**.\n",
    "\n",
    "Essentially what the random seed does is \"flavour\" the randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0655, 0.9338, 0.8605, 0.1228],\n",
      "        [0.4696, 0.0552, 0.2301, 0.3960],\n",
      "        [0.3597, 0.9647, 0.5911, 0.0189]])\n",
      "tensor([[0.2044, 0.0526, 0.8519, 0.7331],\n",
      "        [0.1641, 0.1946, 0.8829, 0.5153],\n",
      "        [0.0631, 0.6547, 0.7178, 0.8623]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# create random tensors\n",
    "\n",
    "random_tensor_A = torch.rand(3,4)\n",
    "random_tensor_B = torch.rand(3,4)\n",
    "print(random_tensor_A)\n",
    "print(random_tensor_B)\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets make some random but reproducible tensors\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# set the random seed\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_C = torch.rand(3,4)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_D = torch.rand(3,4)\n",
    "\n",
    "print(random_tensor_C)\n",
    "print(random_tensor_D)\n",
    "# both tensors should be the same because we set the random seed to the same value\n",
    "# this is useful for reproducibility in machine learning experiments\n",
    "random_tensor_C == random_tensor_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra resources for reproducibility:\n",
    "\n",
    "* https://pytorch.org/docs/stable/notes/randomness.html\n",
    "* https://en.wikipedia.org/wiki/Random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running tensors and PyTorch objects on the GPUs (and making faster computations)\n",
    "GPUs = faster computation on numbers, thanks to CUDA + NVIDIA hardware + PyTorch working behind the scenes to make everything hunky\n",
    "dory (good).\n",
    "\n",
    "\n",
    "### 1. Getting a GPU\n",
    "1. Easiest - Use Google Colab for a free GPU (options to upgrade as well)\n",
    "2. Use your own GPU - takes a little bit of setup and requires the investment of purchasing a GPU, there's lots of options..., see online resources for\n",
    "what options to get\n",
    "3. Use cloud computing - GCP, AWS, Azure, these services allow you to rent computers on the cloud and access them\n",
    "\n",
    "For 2, 3 PyTorch + GPU drivers (CUDA) takes a little bit of setting up, to do this, refer to PyTorch setup documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 26 03:14:10 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.70                 Driver Version: 560.70         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   69C    P0             24W /   80W |    1930MiB /   6144MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2684    C+G   ...0.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A      4408    C+G   ...ata\\Local\\Box\\Box Edit\\Box Edit.exe      N/A      |\n",
      "|    0   N/A  N/A      5296    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      7960    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A      9144    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     10292    C+G   C:\\Windows\\System32\\NahimicSvc64.exe        N/A      |\n",
      "|    0   N/A  N/A     12912    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A     13036    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     13388    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     13660    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     14356    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     15560    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     15628    C+G   ...mpt_builder\\LogiAiPromptBuilder.exe      N/A      |\n",
      "|    0   N/A  N/A     16276    C+G   ...aam7r\\AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A     17232    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     18152    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     19784    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     19836    C+G   ...werToys\\PowerToys.PowerLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     20060    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     20220    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     20280    C+G   ...ys\\WinUI3Apps\\PowerToys.Peek.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     22280    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22544    C+G   ...pIntegrations\\Grammarly.Desktop.exe      N/A      |\n",
      "|    0   N/A  N/A     23188    C+G   ...BridgeWPF\\SamsungNotesBridgeWPF.exe      N/A      |\n",
      "|    0   N/A  N/A     23460    C+G   ...e Stream\\94.0.1.0\\GoogleDriveFS.exe      N/A      |\n",
      "|    0   N/A  N/A     26120    C+G   ...1.0_x64__w2gh52qy24etm\\Nahimic3.exe      N/A      |\n",
      "|    0   N/A  N/A     28884    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A     29292    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     29636    C+G   ...ekyb3d8bbwe\\WsaClient\\WsaClient.exe      N/A      |\n",
      "|    0   N/A  N/A     30200      C   ...ac253\\anaconda3\\envs\\ARC\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     30976    C+G   ....5536.0_x64__8j3eq9eme6ctt\\IGCC.exe      N/A      |\n",
      "|    0   N/A  N/A     31912    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Check for GPU access with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PyTorch since it's capable of running compute on the GPU or CPU, it's best practice to setup device agnostic code: https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "E.g. run on GPU if available, else default to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup device agnostic code\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Putting tensors (and models) on the GPU\n",
    "The reason we want our tensors/models on the GPU is because using a GPU results in faster computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor (deault is GPU)\n",
    "tensor = torch.tensor([1,2,3], device=\"cpu\")\n",
    "\n",
    "# Tensor not on GPU\n",
    "print(tensor, tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], device='cuda:0') cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Move Tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "\n",
    "print(tensor_on_gpu, tensor_on_gpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Moving tensors back to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    }
   ],
   "source": [
    "# Move tensor from GPU to CPU\n",
    "tensor_on_cpu = tensor_on_gpu.to(\"cpu\")\n",
    "print(tensor_on_cpu, tensor_on_cpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Documentation reading - A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework you're using. We'll be using the PyTorch documentation a lot throughout the rest of this course. So I'd recommend spending 10-minutes reading the following (it's okay if you don't get some things for now, the focus is not yet full understanding, it's awareness). See the documentation on torch.Tensor and for torch.cuda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a random tensor with shape (7, 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8694, 0.5677, 0.7411, 0.4294, 0.8854, 0.5739, 0.2666],\n",
       "        [0.6274, 0.2696, 0.4414, 0.2969, 0.8317, 0.1053, 0.2695],\n",
       "        [0.3588, 0.1994, 0.5472, 0.0062, 0.9516, 0.0753, 0.8860],\n",
       "        [0.5832, 0.3376, 0.8090, 0.5779, 0.9040, 0.5547, 0.3423],\n",
       "        [0.6343, 0.3644, 0.7104, 0.9464, 0.7890, 0.2814, 0.7886],\n",
       "        [0.5895, 0.7539, 0.1952, 0.0050, 0.3068, 0.1165, 0.9103],\n",
       "        [0.6440, 0.7071, 0.6581, 0.4913, 0.8913, 0.1447, 0.5315]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor = torch.rand(7,7)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Perform a matrix multiplication on the tensor from 2 with another random tensor with shape (1, 7) (hint: you may have to transpose the second tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1587, 0.6542, 0.3278, 0.6532, 0.3958, 0.9147, 0.2036]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tensor = torch.rand(1,7)\n",
    "new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7]) torch.Size([7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(new_tensor.shape, random_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1587],\n",
      "        [0.6542],\n",
      "        [0.3278],\n",
      "        [0.6532],\n",
      "        [0.3958],\n",
      "        [0.9147],\n",
      "        [0.2036]]) torch.Size([7, 1])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = new_tensor.reshape(7,1)\n",
    "print(new_tensor, new_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9625],\n",
       "        [1.0950],\n",
       "        [0.9967],\n",
       "        [1.8910],\n",
       "        [1.9205],\n",
       "        [1.0674],\n",
       "        [1.6949]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.matmul(new_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Set the random seed to 0 and do exercises 2 & 3 over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4963, 0.7682, 0.0885, 0.1320, 0.3074, 0.6341, 0.4901],\n",
      "        [0.8964, 0.4556, 0.6323, 0.3489, 0.4017, 0.0223, 0.1689],\n",
      "        [0.2939, 0.5185, 0.6977, 0.8000, 0.1610, 0.2823, 0.6816],\n",
      "        [0.9152, 0.3971, 0.8742, 0.4194, 0.5529, 0.9527, 0.0362],\n",
      "        [0.1852, 0.3734, 0.3051, 0.9320, 0.1759, 0.2698, 0.1507],\n",
      "        [0.0317, 0.2081, 0.9298, 0.7231, 0.7423, 0.5263, 0.2437],\n",
      "        [0.5846, 0.0332, 0.1387, 0.2422, 0.8155, 0.7932, 0.2783]])\n",
      "torch.Size([7, 7])\n",
      "tensor([[0.4820, 0.8198, 0.9971, 0.6984, 0.5675, 0.8352, 0.2056]])\n",
      "torch.Size([1, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8542],\n",
       "        [1.9611],\n",
       "        [2.2884],\n",
       "        [3.0481],\n",
       "        [1.7067],\n",
       "        [2.5290],\n",
       "        [1.7989]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor = torch.rand(7,7)\n",
    "print(random_tensor)\n",
    "print(random_tensor.shape)\n",
    "\n",
    "#torch.manual_seed(RANDOM_SEED)\n",
    "new_tensor = torch.rand(1,7)\n",
    "print(new_tensor)\n",
    "print(new_tensor.shape)\n",
    "\n",
    "random_tensor.mm(new_tensor.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Speaking of random seeds, we saw how to set it with torch.manual_seed() but is there a GPU equivalent? (hint: you'll need to look into the documentation for torch.cuda for this one). If there is, set the GPU random seed to 1234."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_RANDOM_SEED = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create two random tensors of shape (2, 3) and send them both to the GPU (you'll need access to a GPU for this). Set torch.manual_seed(1234) when creating the tensors (this doesn't have to be the GPU random seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1272, 0.8167, 0.5440],\n",
      "        [0.6601, 0.2721, 0.9737]], device='cuda:0') cuda:0\n",
      "tensor([[0.6208, 0.0276, 0.3255],\n",
      "        [0.1114, 0.6812, 0.3608]], device='cuda:0') cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.cuda.manual_seed(GPU_RANDOM_SEED)\n",
    "random_tensor_A = torch.rand((2,3),device=device)\n",
    "\n",
    "#torch.cuda.manual_seed(GPU_RANDOM_SEED)\n",
    "random_tensor_B = torch.rand((2,3), device=device)\n",
    "\n",
    "print(random_tensor_A, random_tensor_A.device)\n",
    "print(random_tensor_B, random_tensor_B.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Perform a matrix multiplication on the tensors you created in 6 (again, you may have to adjust the shapes of one of the tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2786, 0.7668],\n",
       "        [0.7343, 0.6102]], device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_multiplication = random_tensor_A.mm(random_tensor_B.T)\n",
    "matrix_multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Find the maximum and minimum values of the output of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2786, device='cuda:0')\n",
      "tensor(0.7668, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(matrix_multiplication))\n",
    "print(torch.max(matrix_multiplication))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Find the maximum and minimum index values of the output of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmin(matrix_multiplication))\n",
    "print(torch.argmax(matrix_multiplication))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Make a random tensor with shape (1, 1, 1, 10) and then create a new tensor with all the 1 dimensions removed to be left with a tensor of shape (10). Set the seed to 7 when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297,\n",
      "           0.3653, 0.8513]]]]) torch.Size([1, 1, 1, 10])\n",
      "tensor([0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251, 0.2071, 0.6297, 0.3653,\n",
      "        0.8513]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 7\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "tensor = torch.rand(1,1,1,10)\n",
    "\n",
    "print(tensor, tensor.shape)\n",
    "\n",
    "squeezed_tensor = tensor.squeeze()\n",
    "print(squeezed_tensor, squeezed_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
